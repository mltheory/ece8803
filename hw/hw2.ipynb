{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "offensive-stanford",
   "metadata": {},
   "source": [
    "## HW 2, Problem 2 -- New algorithms through new regularizers\n",
    "\n",
    "Problem 1 showed that we can recover the multiplicative weights update through the online optimization framework, by minimizing the combination of a data-driven loss term and a regularizer that encourages randomization (the negative entropy function). However, the negative entropy function is not the *only* function that would encourage randomization. We could always choose other regularizers. What algorithm would be obtain from this? Will it be better or worse than multiplicative weights?\n",
    "\n",
    "In this problem, we will empirically explore a different choice of regularizer, and compare its performance to multiplicative weights.\n",
    "\n",
    "We start with some basic imports required for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-louisville",
   "metadata": {},
   "source": [
    "In Problem 1, you showed that multiplicative weights can be written as the solution to the following optimization problem:\n",
    "$$\\hat{P}_t = {\\arg \\min}_{p \\in [0,1]} \\left[f(L_{t-1,1},L_{t-1,0},p) - \\frac{1}{\\eta} H(p)\\right],$$\n",
    "where $H(p)$ was defined as the binary entropy function.\n",
    "\n",
    "Now, we will explore a different type of regularizer, i.e. instead consider the update\n",
    "$$\\tilde{P}_t = {\\arg \\min}_{p \\in [0,1]} \\left[f(L_{t-1,1},L_{t-1,0},p) + \\frac{1}{\\eta} R(p)\\right],$$\n",
    "where we now define\n",
    "$$R(p) := \\log\\left(\\frac{1}{p}\\right) + \\log\\left(\\frac{1}{1 - p}\\right).$$\n",
    "\n",
    "This is often called the \"log-barrier\" regularizer and was actually first proposed by Nemirovski and Yudin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-trust",
   "metadata": {},
   "source": [
    "## Part (a): \n",
    "\n",
    "Plot $R(p)$ versus $p$. Where is it minimized? Where is it maximized? Use this plot to argue that minimizing $R(p)$ encourages randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Space for answer to part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-forty",
   "metadata": {},
   "source": [
    "## Part (b):\n",
    "\n",
    "The optimization problem defined for the log-barrier regularizer turns out to have a closed form solution, which is given by:\n",
    "$$\\tilde{P}_t := f_2(D_t) := \\frac{2}{\\eta D_t + \\sqrt{\\eta^2 D_t^2 + 4} + 2},$$\n",
    "where $D_t := L_{t-1,1} - L_{t-1,0}$, i.e. the difference in the cumulative losses.\n",
    "\n",
    "Express the multiplicative weight update at time step $t$ as a different function $\\hat{P}_t = f_1(D_t)$. Then, plot both functions $f_1(\\cdot)$ and $f_2(\\cdot)$ as a function of $d$ ranging between $-10$ and $10$ for the case $\\eta = 1$. Which function decreases faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Space for answer to part (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-princess",
   "metadata": {},
   "source": [
    "We will now compare the performance of the two algorithms defined by $\\hat{P}_t$ and $\\tilde{P}_t$.\n",
    "First, we define some sequences and some preliminaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateIIDSequence(T, p):\n",
    "    \"\"\"\n",
    "    Code that generates an iid sequence of length T with Bernoulli parameter p.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.random.binomial(1, p, T)\n",
    "\n",
    "def GenerateOnePeriodicSequence(T):\n",
    "    \"\"\"\n",
    "    Code that generates a 1-periodic sequence of length T.\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_list = [1,0]*int(T/2)\n",
    "    return np.array(sequence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-accounting",
   "metadata": {},
   "source": [
    "## Part (c): \n",
    "\n",
    "Implement the multiplicative weights update and the log-barrier update here as a function of the losses $L_{t-1,0},L_{t-1,1}$. In the starter code provided below, the variable \"losses\" is a numpy array with $2$ entries given by $[L_{t-1,0},L_{t-1,1}]$. Please make the output of your algorithm a numpy array \"prob\" with $2$ entries, given by $[P_{t,0},P_{t,1}]$.\n",
    "\n",
    "You are free to refer to (and borrow from) the code provided in the resources \"MultiplicativeWeights.ipynb\" (note that this was defined for rewards, so will require slight changes).\n",
    "\n",
    "Hint for log-barrier: recall that we defined $D_t = L_{t-1,1}- L_{t-1,0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiplicativeWeights(losses,eta):\n",
    "    \"\"\"\n",
    "    Implementation of multiplicative weight update, eta is the step size.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Space for implementation\n",
    "    \n",
    "def LogBarrierUpdate(losses,eta):\n",
    "    \"\"\"\n",
    "    Implementation of log barrier update.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Space for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-intellectual",
   "metadata": {},
   "source": [
    "## Part (d):\n",
    "\n",
    "Now, we compare the performance of both algorithms on the two running examples that we have been considering in class.\n",
    "We set $T = 600$, and consider two types of sequences: \n",
    "\n",
    "a) the case where $X_t = 1$ for all $t = 1,\\ldots,T$.\n",
    "b) the case of the $1$-periodic sequence.\n",
    "\n",
    "For both algorithms, we will set $\\eta = 1$ in order to observe a more dramatic visualization in the difference in performance.\n",
    "\n",
    "Plot the total *loss* of each algorithm (what we defined as $H_t$ in class) versus the number of time steps $t$ on sequences (a) and (b). Please make separate figures for the evaluation of (a) and (b).\n",
    "\n",
    "You are welcome to directly use the starter code provided below to evaluate the performance of an arbitrary algorithm (parameterized by eta) on an arbitrary binary sequence. This starter code will return a \"total loss vector\" given by $\\begin{bmatrix} H_1 & \\ldots H_T \\end{bmatrix}$ for plotting convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePerformance(Alg, sequence, eta):\n",
    "    \"\"\"\n",
    "    Starter code provided to evaluate the performance of an algorithm on a sequence. Here, \"Alg\" is a function handle.\n",
    "    You can either use this code directly, or borrow from it to create separate functions to evaluate performance of the two algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    T = len(sequence)\n",
    "    \n",
    "    total_loss_alg = 0 #denotes total loss of algorithm at current round\n",
    "    total_loss_vector_alg = np.zeros(T) #denotes vector of running losses, what will be returned by the function\n",
    "    total_loss_actual = np.zeros(2) #denotes total loss of each letter (0 or 1) at current round\n",
    "    for t in range(T):\n",
    "        total_loss_vector_alg[t] = total_loss_alg\n",
    "        prob = Alg(total_loss_actual, eta)\n",
    "        total_loss_alg += prob[0]*sequence[t] + prob[1]*(1 - sequence[t]) #0-1 loss function evaluation.\n",
    "        total_loss_actual[0] += (sequence[t] == 1)\n",
    "        total_loss_actual[1] += (sequence[t] == 0)\n",
    "    \n",
    "    return total_loss_vector_alg\n",
    "\n",
    "\n",
    "###Space for specifying T and eta, creating sequence (a) and (b), and evaluating performance of both algorithms on these sequences.\n",
    "\n",
    "###Space for plotting H_t v.s. t for both algorithms on sequences (a) and (b) respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-terrorism",
   "metadata": {},
   "source": [
    "## Part (e):\n",
    "\n",
    "Report the superior algorithm for sequence (a) and sequence (b) respectively.\n",
    "Based on this report, which algorithm do you think tends to randomize more?\n",
    "(You are also welcome to use the answer to part (b) as a hint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-friend",
   "metadata": {},
   "source": [
    "## Theory bonus (part (f)):\n",
    "\n",
    "Prove the closed-form expression for $\\tilde{P}_t$ that was provided in part (b). \n",
    "\n",
    "Hint: You may find the quadratic formula useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
